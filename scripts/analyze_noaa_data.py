# Run unpack_noaa_data.py before running this.
# Data must be located in file path indicated by noaa_amy_files_path directory.
# Will ignore NOAA files for the current year (which will be incomplete).

import os
import argparse
import pandas as pd
from datetime import datetime
from glob import iglob

output_dir_path = '../outputs/analyze_noaa_data_output'
files_to_convert_csv_path = os.path.join(output_dir_path, 'files_to_convert.csv')

parser = argparse.ArgumentParser(
    description=f"""
        Perform an analysis of the files generated by unpack_noaa_data.py, which must be run before this script. The
        result of the analysis will be written to {files_to_convert_csv_path}.
    """
)
parser.add_argument('--max-missing-rows',
                    default=700,
                    type=int,
                    help='ISD files with more than this number of missing rows will be excluded from the output')
parser.add_argument('--max-consecutive-missing-rows',
                    default=48,
                    type=int,
                    help='ISD files with more than this number of consecutive missing rows will be excluded from the output')
args = parser.parse_args()

# Identify the current year
current_year = str(datetime.now().year)

# Initialize the dfs for the files that will be created.
missing_total_entries_high = pd.DataFrame(columns=['file', 'total_rows_missing'])
missing_consec_entries_high = pd.DataFrame(columns=['file', 'total_rows_missing', 'max_consec_rows_missing'])
files_to_convert = pd.DataFrame(columns=['file', 'total_rows_missing', 'max_consec_rows_missing'])

# Initialize counters for how many files have been processed and skipped.
files_processed = 0
files_skipped = 0

# Make a directory to store results if it doesn't already exist.
if not os.path.exists(output_dir_path):
    os.makedirs(output_dir_path)

# Loop through all .gz files in the input directory - we filter on extension so that things like
# README files can be in that path without causing any problems. We filter on files that begin
# with 7 because all NOAA ISD Lite files for North America should start with 7, as all WMO indices
# in North America do.
for file in iglob('../inputs/NOAA_ISD_Lite_Raw/**/7*.gz'):
    # Skip current year's files because they are probably incomplete
    if file.endswith(current_year + ".gz"):
        print(file + ": skipping file because it is from the current year")
        files_skipped += 1
        continue
    else:
        print(file + ": Processing")
        files_processed += 1

    # Read the file into a Pandas dataframe.
    df = pd.read_csv(file,
                     delim_whitespace=True,
                     header=None,
                     compression="gzip",
                     names=["Year", "Month", "Day", "Hour", "Air_Temperature",
                            "Dew_Point_Temperature", "Sea_Level_Pressure", "Wind_Direction",
                            "Wind_Speed_Rate", "Sky_Condition_Total_Coverage_Code",
                            "Liquid_Precipitation_Depth_Dimension_1H", "Liquid_Precipitation_Depth_Dimension_6H"]
    )

    # Take year-month-day-hour columns and convert to datetime stamps.
    df['obs_timestamps'] = pd.to_datetime(pd.DataFrame({'year': df['Year'],
                                                        'month': df['Month'],
                                                        'day': df['Day'],
                                                        'hour': df['Hour']}))

    # Remove unnecessary year, month, day, hour columns
    df = df.drop(columns=['Year', 'Month', 'Day', 'Hour'])

    # Identify files with too many total rows missing.
    rows_present = df.shape[0]
    rows_missing = 8760 - rows_present
    if rows_missing > args.max_missing_rows:
        missing_total_entries_high = missing_total_entries_high.append(
            {'file': file, 'total_rows_missing' : rows_missing}, ignore_index=True)

    # Identify files with too many consecutive rows missing.
    else:
        maxcounter = 0

        # Skip all this work if the total number of missing rows isn't at least as much as the maximum number of
        # consecutive missing rows: You can't be missing 10 consecutive rows if you're not missing 10 total
        if rows_missing > args.max_consecutive_missing_rows:
            # Create series of continuous timestamp values for that year
            all_timestamps = pd.date_range(df['obs_timestamps'].iloc[0], periods=8760, freq='H')

            # Merge to one dataframe containing all continuous timestamp values.
            all_times = pd.DataFrame(all_timestamps, columns=['all_timestamps'])
            df_all_times = pd.merge(all_times, df, how='left', left_on='all_timestamps', right_on='obs_timestamps')

            # Create series of only the missing timestamp values
            missing_times = df_all_times[df_all_times.isnull().any(axis=1)]
            missing_times = missing_times['all_timestamps']

            # Create a series containing the time step distance from the previous timestamp for the missing timestamp values
            missing_times_diff = missing_times.diff()

            # Count the maximum number of consecutive missing time steps.
            counter = 1

            for step in missing_times_diff:
                if step == pd.Timedelta('1h'):
                    counter += 1
                    if counter > maxcounter:
                        maxcounter = counter
                elif step > pd.Timedelta('1h'):
                    counter = 0

        if maxcounter > args.max_consecutive_missing_rows:
            missing_consec_entries_high = missing_consec_entries_high.append(
                {'file': file, 'total_rows_missing' : rows_missing, 'max_consec_rows_missing': maxcounter},
                ignore_index=True)

        # The file is not missing too many rows total or too many in a row: Add it to the set of files to
        # be converted to AMY EPWs.
        else:
            files_to_convert = files_to_convert.append(
                {'file': file, 'total_rows_missing' : rows_missing, 'max_consec_rows_missing': maxcounter},
                ignore_index=True)

# Write the dataframes to CSVs for the output files.
if not missing_total_entries_high.empty:
    missing_total_entries_high_file_path = os.path.join(output_dir_path, 'missing_total_entries_high.csv')
    print(
        len(missing_total_entries_high),
        "records excluded because they were missing more than", args.max_missing_rows,
        "rows. Information about these files will be written to", missing_total_entries_high_file_path
    )
    missing_total_entries_high.to_csv(missing_total_entries_high_file_path, index=False)

if not missing_consec_entries_high.empty:
    missing_consec_entries_high_file_path = os.path.join(output_dir_path, 'missing_consec_entries_high.csv')
    print(
        len(missing_consec_entries_high),
        "records excluded because they were missing more than", args.max_consecutive_missing_rows,
        "consecutive rows. Information about these files will be written to", missing_consec_entries_high_file_path
    )
    missing_consec_entries_high.to_csv(missing_consec_entries_high_file_path, index=False)

files_to_convert.to_csv(files_to_convert_csv_path, index=False)

print('total files: ', files_processed + files_skipped)
print('files processed: ', str(files_processed))
print('files skipped: ', str(files_skipped))